{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the RoDeP doc ! \u00b6 The following documentation is related to the RoDeP project, realised in collaboration between Polytech Sorbonne and Sony CSL Paris. You will find the building instructions, and the manual for using the robot.","title":"Home"},{"location":"#welcome-to-the-rodep-doc","text":"The following documentation is related to the RoDeP project, realised in collaboration between Polytech Sorbonne and Sony CSL Paris. You will find the building instructions, and the manual for using the robot.","title":"Welcome to the RoDeP doc !"},{"location":"about/","text":"About the RoDeP project \u00b6 The RoDeP (standing for \"RObot DEplaceur de Plantes\" in French, which can be translated by PMORO (\"Plant MOver RObot\")) project is a collaboration between Sony CSL Paris and Polytech Sorbonne, a graduating school of engineering. It has be given to students of Polytech Sorbonne in the Robotic specialty, as their indusrial project, realised in cooperation with this company. Sony CSL Paris developped a plant scanner, which makes 3D model of vegetable, in order to track their growth. Project funding \u00b6 The project has been funded by Sony CSL entirely. Work team \u00b6 3 teams participated in this project. The first is the student team, including Nicolas BRANAS, Paul CHAMBROUX, Alexandre LEU and Hugo PASQUIER. It has been developped over the course of their 4th year in Polytech Sorbonne. The second team, composed by David COLLIAUX and Peter HANAPPE, is representing Sony CSL. Both of them works in the Sustainability department, which is \"commited to build tools for a sustainable society and raise awareness about the challenges of our era\". Last but not least, the Polytech Sorbonne team, with Pierre CARLES, supervised the students while they were at the school, on the technical side of the project. GitHub sources \u00b6 You will find every ressources for the code part of this project on the Github down below :","title":"About"},{"location":"about/#about-the-rodep-project","text":"The RoDeP (standing for \"RObot DEplaceur de Plantes\" in French, which can be translated by PMORO (\"Plant MOver RObot\")) project is a collaboration between Sony CSL Paris and Polytech Sorbonne, a graduating school of engineering. It has be given to students of Polytech Sorbonne in the Robotic specialty, as their indusrial project, realised in cooperation with this company. Sony CSL Paris developped a plant scanner, which makes 3D model of vegetable, in order to track their growth.","title":"About the RoDeP project"},{"location":"about/#project-funding","text":"The project has been funded by Sony CSL entirely.","title":"Project funding"},{"location":"about/#work-team","text":"3 teams participated in this project. The first is the student team, including Nicolas BRANAS, Paul CHAMBROUX, Alexandre LEU and Hugo PASQUIER. It has been developped over the course of their 4th year in Polytech Sorbonne. The second team, composed by David COLLIAUX and Peter HANAPPE, is representing Sony CSL. Both of them works in the Sustainability department, which is \"commited to build tools for a sustainable society and raise awareness about the challenges of our era\". Last but not least, the Polytech Sorbonne team, with Pierre CARLES, supervised the students while they were at the school, on the technical side of the project.","title":"Work team"},{"location":"about/#github-sources","text":"You will find every ressources for the code part of this project on the Github down below :","title":"GitHub sources"},{"location":"assembly/","text":"Prepare your components \u00b6 Be sure having the whole components, they are enumerate in the component list . 1. Mount the frame \u00b6 In order to assemble the frame, you have to gather the following elements : Number Component 1 frame 2 wheel 2 wheel motor 2 roulette 2 battery wedge In the end, you should have something like this :","title":"Assembly"},{"location":"assembly/#prepare-your-components","text":"Be sure having the whole components, they are enumerate in the component list .","title":"Prepare your components"},{"location":"assembly/#1-mount-the-frame","text":"In order to assemble the frame, you have to gather the following elements : Number Component 1 frame 2 wheel 2 wheel motor 2 roulette 2 battery wedge In the end, you should have something like this :","title":"1. Mount the frame"},{"location":"component/","text":"Bill Of Material \u00b6 This chart makes an inventory of all the elements you need to build the robot. Number Component Specifications Example 1 Battery LIFEPO4 12V BMS Protection Power Battery 1 Charger NOCO GENIUS2 2 Motor driver Shield for Arduino gShield V5 2 Gripper motor Step motor Motor 2 Wheel motor Step motor Motor Encodeurs???? 1 Voltage converter 12V - 5V Rail Din adapted Voltage converter 1 Camera Raspicam 1 LED Raspicam adapted LED LED lisiparoi 3 Rail DIN fixation Rail DIN fixation 2 Controlleur Arduino uno or equivalent Arduino Uno 1 Singel board computeur Raspberry Pi 3B or equivalent Raspberry Pi 3B 2 Connector strip 5-pole Wago 2 Rail DIN's fixation for wago Fixation for wago 2 Stepper Mounting Bracket NEMA 23 1 Frame 2 Wheel 2 Roulette 2 Battery wedge","title":"Components list"},{"location":"component/#bill-of-material","text":"This chart makes an inventory of all the elements you need to build the robot. Number Component Specifications Example 1 Battery LIFEPO4 12V BMS Protection Power Battery 1 Charger NOCO GENIUS2 2 Motor driver Shield for Arduino gShield V5 2 Gripper motor Step motor Motor 2 Wheel motor Step motor Motor Encodeurs???? 1 Voltage converter 12V - 5V Rail Din adapted Voltage converter 1 Camera Raspicam 1 LED Raspicam adapted LED LED lisiparoi 3 Rail DIN fixation Rail DIN fixation 2 Controlleur Arduino uno or equivalent Arduino Uno 1 Singel board computeur Raspberry Pi 3B or equivalent Raspberry Pi 3B 2 Connector strip 5-pole Wago 2 Rail DIN's fixation for wago Fixation for wago 2 Stepper Mounting Bracket NEMA 23 1 Frame 2 Wheel 2 Roulette 2 Battery wedge","title":"Bill Of Material"},{"location":"control/","text":"Controlling the RoDeP \u00b6 The RoDeP\u2019s controls are coded in Arduino code, but its routines are coded as a state machine, using the StateMachine library from Python, for easier modifications. Here we detail its use, and give some tips about how tuning this code for your use. The main problem being the line following process, we will have a deeper look on its fine tuning. The state machine \u00b6 The RoDeP state machine has the purpose of being reasonably understandable and tunable. It defines the robot as an object composed of a navigation controller (Arduino with drivers for controlling navigation motors), a gripper controller (Arduino with drivers for controlling the pliers), a camera and other useful parameters, such as masks for image treatment. With this format, the robot goes through 9 states during its mission, two of them being the initial state and final state. When starting the script, make sure to specify the number of plants to be scanned. We tried to define those states as intuitive as possible. Here is a diagram outlining its routine : add picture : routine Here is the detail of every state : Initializing : transitory state. At the start of the program, it initialize the serial ports for the Arduino controllers, warm up the camera, prepares the pliers, etc\u2026 It also creates, if not already existing, a log-folder that reports the line following error overt time, in case of testing. Seeking : the robot follows the green line, looking for the right AprilTag to stop. Taking : the robot closes its pliers, lift up the plant, go back a little and make a half-turn. Going to scanner : following the green line again, the robot stops at scanner, lifting the plant during its journey. Waiting for scan completion : simply waits that the scan is done to start the next state. Going to storage : the robot makes a half turn and follows the blue line, stopping at the right tag for the actual plant, the pliers going down in the process. Putting down : the robot finish to put down its pliers and then release its grip. Going back to scanner : the robot go back a little, makes a half turn and follows the blue line to the scanner. It makes a half turn there, and enter its mission complete state if it has scanned the number of plants specified, otherwise it goes back to the seeking state. Mission complete : the pliers close a last time, and a end-of-mission message is displayed as a notification. Fine tuning the proportional controller \u00b6 At the moment, the navigation algorithm is controlled using a proportional controller. It suits our actual uses (2 parallel straight lines with 2 turns to the scanner; see figures about the route, average speed of 300 increments per seconds) but can be tuned for your situation, for instance increase the speed. Just keep in mind that the system converges naturally to the \u201cerror = 0\u201d state, so adding an integral gain would just slow the system; adding a derivative gain would prove useful for cancelling remaining oscillations, especially if you want to re-design the route to add more curves or increase speed, but if you get high amplitude oscillations, implementing a low-pass filter is advised, in order to cancel high-frequency noise that would be amplified by the derivative. As a reminder, when the camera takes a picture, it crops it to approximately the second quarter (from the top), and we estimate the error as the distance (on the x-axis) between the center of the cropped image and the mean value of the remaining black pixels, after image filtering. add picture viewing zone We then multiply this error value with the proportional gain, and pass the resulting value to a differential drive for the motors : add picture corrector As you can see, the speed given in each motor is equal to a certain proportion of the average speed of the wheels, in increments per seconds. In order to finetune the proportional gain, we give a set of tests we did in different situations. We tested 6 values in 3 different starting points, and recommend you to use the same starts in order to compare your own plots of error with ours. The graphs we give focus on the error over time regarding the line following algorithm, but beside this quantified quality, it is important to keep an acute eye over two qualitative parameters : first, the orientation of the robot at the end of the test (are the pliers in front of the plant it should be taking ?), and secondly the \u201cempiric oscillations\u201d of the robot. As the error is measured in pixels, oscillations appearing on the graphs might not even be detectable as a human, and therefore can be insignificant. The first set of tests \u00b6 For our first set of tests, we gave the robot a step of error, and aligned it with the line it is supposed to follow, as shown on the following drawing : picture situation Testing 6 gains with this initial state gave the following results : picture results Only one gain makes the system divergent. The others complete their task with several levels of satisfaction : though not divergent, the lowest gain of 0.001 pixels^-1 is too slow and doesn\u2019t even reach closely the order on a satisfying time, regarding our system. The gain of 0.015 pixels^-1 is also not satisfying, as it oscillates too frequently. The second set of tests \u00b6 Step of error, unaligned robot : picture situation With those tests, we evaluate the capacity of the robot to align with the line and the time it takes to do it. picture results As expected, the lowest gain tested is too low to prevent the robot from losing the line, and diverges quickly. The gain of 0.015 pixels^-1 is still unsatisfying as the navigation enters in a \u201cpseudo-oscillatory state\u201d, and as the robot is not aligned with the line once it gets to its objective. On the scale of the system, the gain of 0.007 pixels^-1 is a good compromise between converging speed, alignment with the objective and remaining oscillations. The third set of tests \u00b6 Robot going from the scanner to the first plant. add picture sitatuin With those tests, we evaluate the capacity of the robot at reaching its effective objective in a \u201creal situation\u201d. add picture results As previously mentioned, the gain of 0.007 pixels^-1 is a good choice since it makes the robot getting to its objective correctly, with a good average time.","title":"Motor control"},{"location":"control/#controlling-the-rodep","text":"The RoDeP\u2019s controls are coded in Arduino code, but its routines are coded as a state machine, using the StateMachine library from Python, for easier modifications. Here we detail its use, and give some tips about how tuning this code for your use. The main problem being the line following process, we will have a deeper look on its fine tuning.","title":"Controlling the RoDeP"},{"location":"control/#the-state-machine","text":"The RoDeP state machine has the purpose of being reasonably understandable and tunable. It defines the robot as an object composed of a navigation controller (Arduino with drivers for controlling navigation motors), a gripper controller (Arduino with drivers for controlling the pliers), a camera and other useful parameters, such as masks for image treatment. With this format, the robot goes through 9 states during its mission, two of them being the initial state and final state. When starting the script, make sure to specify the number of plants to be scanned. We tried to define those states as intuitive as possible. Here is a diagram outlining its routine : add picture : routine Here is the detail of every state : Initializing : transitory state. At the start of the program, it initialize the serial ports for the Arduino controllers, warm up the camera, prepares the pliers, etc\u2026 It also creates, if not already existing, a log-folder that reports the line following error overt time, in case of testing. Seeking : the robot follows the green line, looking for the right AprilTag to stop. Taking : the robot closes its pliers, lift up the plant, go back a little and make a half-turn. Going to scanner : following the green line again, the robot stops at scanner, lifting the plant during its journey. Waiting for scan completion : simply waits that the scan is done to start the next state. Going to storage : the robot makes a half turn and follows the blue line, stopping at the right tag for the actual plant, the pliers going down in the process. Putting down : the robot finish to put down its pliers and then release its grip. Going back to scanner : the robot go back a little, makes a half turn and follows the blue line to the scanner. It makes a half turn there, and enter its mission complete state if it has scanned the number of plants specified, otherwise it goes back to the seeking state. Mission complete : the pliers close a last time, and a end-of-mission message is displayed as a notification.","title":"The state machine"},{"location":"control/#fine-tuning-the-proportional-controller","text":"At the moment, the navigation algorithm is controlled using a proportional controller. It suits our actual uses (2 parallel straight lines with 2 turns to the scanner; see figures about the route, average speed of 300 increments per seconds) but can be tuned for your situation, for instance increase the speed. Just keep in mind that the system converges naturally to the \u201cerror = 0\u201d state, so adding an integral gain would just slow the system; adding a derivative gain would prove useful for cancelling remaining oscillations, especially if you want to re-design the route to add more curves or increase speed, but if you get high amplitude oscillations, implementing a low-pass filter is advised, in order to cancel high-frequency noise that would be amplified by the derivative. As a reminder, when the camera takes a picture, it crops it to approximately the second quarter (from the top), and we estimate the error as the distance (on the x-axis) between the center of the cropped image and the mean value of the remaining black pixels, after image filtering. add picture viewing zone We then multiply this error value with the proportional gain, and pass the resulting value to a differential drive for the motors : add picture corrector As you can see, the speed given in each motor is equal to a certain proportion of the average speed of the wheels, in increments per seconds. In order to finetune the proportional gain, we give a set of tests we did in different situations. We tested 6 values in 3 different starting points, and recommend you to use the same starts in order to compare your own plots of error with ours. The graphs we give focus on the error over time regarding the line following algorithm, but beside this quantified quality, it is important to keep an acute eye over two qualitative parameters : first, the orientation of the robot at the end of the test (are the pliers in front of the plant it should be taking ?), and secondly the \u201cempiric oscillations\u201d of the robot. As the error is measured in pixels, oscillations appearing on the graphs might not even be detectable as a human, and therefore can be insignificant.","title":"Fine tuning the proportional controller"},{"location":"control/#the-first-set-of-tests","text":"For our first set of tests, we gave the robot a step of error, and aligned it with the line it is supposed to follow, as shown on the following drawing : picture situation Testing 6 gains with this initial state gave the following results : picture results Only one gain makes the system divergent. The others complete their task with several levels of satisfaction : though not divergent, the lowest gain of 0.001 pixels^-1 is too slow and doesn\u2019t even reach closely the order on a satisfying time, regarding our system. The gain of 0.015 pixels^-1 is also not satisfying, as it oscillates too frequently.","title":"The first set of tests"},{"location":"control/#the-second-set-of-tests","text":"Step of error, unaligned robot : picture situation With those tests, we evaluate the capacity of the robot to align with the line and the time it takes to do it. picture results As expected, the lowest gain tested is too low to prevent the robot from losing the line, and diverges quickly. The gain of 0.015 pixels^-1 is still unsatisfying as the navigation enters in a \u201cpseudo-oscillatory state\u201d, and as the robot is not aligned with the line once it gets to its objective. On the scale of the system, the gain of 0.007 pixels^-1 is a good compromise between converging speed, alignment with the objective and remaining oscillations.","title":"The second set of tests"},{"location":"control/#the-third-set-of-tests","text":"Robot going from the scanner to the first plant. add picture sitatuin With those tests, we evaluate the capacity of the robot at reaching its effective objective in a \u201creal situation\u201d. add picture results As previously mentioned, the gain of 0.007 pixels^-1 is a good choice since it makes the robot getting to its objective correctly, with a good average time.","title":"The third set of tests"},{"location":"electriccircuit/","text":"Electrical diagram \u00b6 Here is an electrical diagram representing the principle of the electrical plan. Fritzing document \u00b6 Here is a Fritzing capture, the Fritzing file have to be updated, but you still can study the drivers connection. Avaible on the Github. Steppers motors \u00b6 We choose steppers motors for their bigger accuracy. Usefull for helping the line following algorith and griper control. Their consumption is bigger than DC motors in part because they consume as when they are static, they control their holding in position.","title":"Electric circuit"},{"location":"electriccircuit/#electrical-diagram","text":"Here is an electrical diagram representing the principle of the electrical plan.","title":"Electrical diagram"},{"location":"electriccircuit/#fritzing-document","text":"Here is a Fritzing capture, the Fritzing file have to be updated, but you still can study the drivers connection. Avaible on the Github.","title":"Fritzing document"},{"location":"electriccircuit/#steppers-motors","text":"We choose steppers motors for their bigger accuracy. Usefull for helping the line following algorith and griper control. Their consumption is bigger than DC motors in part because they consume as when they are static, they control their holding in position.","title":"Steppers motors"},{"location":"gripp/","text":"Gripper components \u00b6 On this page, you will find every components you need to assemble everything related to the gripper. Support of the gripper \u00b6 First of all, the components for the support part is here : Number Component Specifications Example 4 linear guide length : 34,5 cm, diameter 4 mm 1 Bottom Plate Custom made 1 Top plate Custom made 1 Gripper motor Step motor Motor 1 Worm screw length : cm, diameter 10 Attaches axes/plaques / clamps ? 38 M4x12 Screw 4 washer 2 guides pour vis sans fin // worm screw guide 1 small cogwheel 1 big cogwheel The gripping part \u00b6 The list of the elements found on the gripping part is below : Number Component Specifications Example 2 linear guide f g 4 Attaches length 25 cm 1 Gripper motor Step motor Motor 1 belt length 2 cogwheel 2 clamping jaw 4 Anneaux / rings 4 circlips / rings 1 limit switches 3 Grande vis / screw 2 binding for linear guide 1 binding for worm screw 14 fixing bolt 4 small screw","title":"Gripper system"},{"location":"gripp/#gripper-components","text":"On this page, you will find every components you need to assemble everything related to the gripper.","title":"Gripper components"},{"location":"gripp/#support-of-the-gripper","text":"First of all, the components for the support part is here : Number Component Specifications Example 4 linear guide length : 34,5 cm, diameter 4 mm 1 Bottom Plate Custom made 1 Top plate Custom made 1 Gripper motor Step motor Motor 1 Worm screw length : cm, diameter 10 Attaches axes/plaques / clamps ? 38 M4x12 Screw 4 washer 2 guides pour vis sans fin // worm screw guide 1 small cogwheel 1 big cogwheel","title":"Support of the gripper"},{"location":"gripp/#the-gripping-part","text":"The list of the elements found on the gripping part is below : Number Component Specifications Example 2 linear guide f g 4 Attaches length 25 cm 1 Gripper motor Step motor Motor 1 belt length 2 cogwheel 2 clamping jaw 4 Anneaux / rings 4 circlips / rings 1 limit switches 3 Grande vis / screw 2 binding for linear guide 1 binding for worm screw 14 fixing bolt 4 small screw","title":"The gripping part"},{"location":"hardware_starting/","text":"Building the robot \u00b6 Building order \u00b6 The RoDeP build can be separate in 2 parts : the frame and the gripper. The first one has most of the electronics, with the battery and the boards, the second has the gripping system, used for the elevation of the plant. Therefore, both part can be assembled simulteanously, and bind together at a later point of the project. Building the frame \u00b6 As previously mentionned, the frame has most of the electronics systems of the robot. You can find the instructions in the next part. ![Rail DIN](RailDIN files/rail_DIN.png) Building the gripper \u00b6 The gripper allows the RoDeP to grab the plants, elevate them in order to scan. The instructions are found after the frame.","title":"Starting instruction"},{"location":"hardware_starting/#building-the-robot","text":"","title":"Building the robot"},{"location":"hardware_starting/#building-order","text":"The RoDeP build can be separate in 2 parts : the frame and the gripper. The first one has most of the electronics, with the battery and the boards, the second has the gripping system, used for the elevation of the plant. Therefore, both part can be assembled simulteanously, and bind together at a later point of the project.","title":"Building order"},{"location":"hardware_starting/#building-the-frame","text":"As previously mentionned, the frame has most of the electronics systems of the robot. You can find the instructions in the next part. ![Rail DIN](RailDIN files/rail_DIN.png)","title":"Building the frame"},{"location":"hardware_starting/#building-the-gripper","text":"The gripper allows the RoDeP to grab the plants, elevate them in order to scan. The instructions are found after the frame.","title":"Building the gripper"},{"location":"manual/","text":"","title":"Manual"},{"location":"rail/","text":"Rail DIN system \u00b6 A Rail DIN is a standardized support profile, generally metallic, widely used for the mechanical support of electrical equipment. The Rail DIN system allows us to assemble and disassemble easily the differents electronicals components. Moreover, it offers us the possibility of changing the components whithout having to modifie the frame. Rail DIN support and adaptator \u00b6 To fix the different processor and controlor on the Rail DIN we use support, they allow a strong fixation avoiding all parasyte movement. Then we use homemade plastic adaptator to bind devices and supports. The assembly is complex, do not tighten the bolts too hard the first time or use pcb spacer/support","title":"Rail DIN system"},{"location":"rail/#rail-din-system","text":"A Rail DIN is a standardized support profile, generally metallic, widely used for the mechanical support of electrical equipment. The Rail DIN system allows us to assemble and disassemble easily the differents electronicals components. Moreover, it offers us the possibility of changing the components whithout having to modifie the frame.","title":"Rail DIN system"},{"location":"rail/#rail-din-support-and-adaptator","text":"To fix the different processor and controlor on the Rail DIN we use support, they allow a strong fixation avoiding all parasyte movement. Then we use homemade plastic adaptator to bind devices and supports. The assembly is complex, do not tighten the bolts too hard the first time or use pcb spacer/support","title":"Rail DIN support and adaptator"},{"location":"software/","text":"","title":"Software"},{"location":"software_starting/","text":"","title":"Starting instruction"},{"location":"starting_instruction/","text":"Starting Instructions \u00b6 In order to see a working robot, you will need to follow those steps. 1 : Building the robot \u00b6 Building the robot is the first step to ensure the development of the project. 2 : Coding \u00b6 Now is the time to add the ![code](software.md) to the robot, so it would follow the line and take the plants. 3 : Testing the robot \u00b6 If everything is ready, now all you need is to try the robot !","title":"Starting instruction"},{"location":"starting_instruction/#starting-instructions","text":"In order to see a working robot, you will need to follow those steps.","title":"Starting Instructions"},{"location":"starting_instruction/#1-building-the-robot","text":"Building the robot is the first step to ensure the development of the project.","title":"1 : Building the robot"},{"location":"starting_instruction/#2-coding","text":"Now is the time to add the ![code](software.md) to the robot, so it would follow the line and take the plants.","title":"2 : Coding"},{"location":"starting_instruction/#3-testing-the-robot","text":"If everything is ready, now all you need is to try the robot !","title":"3 : Testing the robot"},{"location":"tracker/","text":"Purpose of the line tracker \u00b6 The line tracker allows the robot to move autonomously in its storages areas. The robot is able to follow a line and recognize Apriltags to stop when it is necessary. The RODEP's work area \u00b6 Here is an example of the orgazination of the robot's work area : Line tracker operation \u00b6 The Raspberry Pi Camera allows to obtain RGB images. These images are passed to HSV format in order to permform a thresholding on them thanks to OpenCV. The principle of thresholding is to set a minimum and a maximum for the value of H, S and V. For each pixel in HSV image, the function test if the value of H, S and V are between the minimum and the minimum. If it is the case then the pixel turns to white otherwise it turns to black. Here is 3 lines of the Python code to obtain the 3 mask with the thresholding : apriltag_mask = 255-cv2.inRange(frame_HSV, (140,110,0), (180,200,255)) blue_line_mask = 255-cv2.inRange(frame_HSV, (110,170,0), (130,255,255)) green_line_mask = 255-cv2.inRange(frame_HSV, (80,60,0), (110,170,255)) For the next step it is neccessary to have the objects in black on a white background. So, this is the reason of the 255-cv2.inrange(...) to invert the colors. Here are examples of masks obtained with the thresholding : Error calculation \u00b6 Now that the lines are detected, the error must be defined and calculated to allow the robot to modifify its trajectory. The chosen line tracking technique is the one where we take the average of the pixels belonging to the line and we want to realign this average with the center of the image. So, the calculated error is the difference between the average of the pixels belonging to the line and the center of the image. The following scheme represent and define the error : This error is used for the motor control. The operation of the motor control is explained on the next page.","title":"Line tracker"},{"location":"tracker/#purpose-of-the-line-tracker","text":"The line tracker allows the robot to move autonomously in its storages areas. The robot is able to follow a line and recognize Apriltags to stop when it is necessary.","title":"Purpose of the line tracker"},{"location":"tracker/#the-rodeps-work-area","text":"Here is an example of the orgazination of the robot's work area :","title":"The RODEP's work area"},{"location":"tracker/#line-tracker-operation","text":"The Raspberry Pi Camera allows to obtain RGB images. These images are passed to HSV format in order to permform a thresholding on them thanks to OpenCV. The principle of thresholding is to set a minimum and a maximum for the value of H, S and V. For each pixel in HSV image, the function test if the value of H, S and V are between the minimum and the minimum. If it is the case then the pixel turns to white otherwise it turns to black. Here is 3 lines of the Python code to obtain the 3 mask with the thresholding : apriltag_mask = 255-cv2.inRange(frame_HSV, (140,110,0), (180,200,255)) blue_line_mask = 255-cv2.inRange(frame_HSV, (110,170,0), (130,255,255)) green_line_mask = 255-cv2.inRange(frame_HSV, (80,60,0), (110,170,255)) For the next step it is neccessary to have the objects in black on a white background. So, this is the reason of the 255-cv2.inrange(...) to invert the colors. Here are examples of masks obtained with the thresholding :","title":"Line tracker operation"},{"location":"tracker/#error-calculation","text":"Now that the lines are detected, the error must be defined and calculated to allow the robot to modifify its trajectory. The chosen line tracking technique is the one where we take the average of the pixels belonging to the line and we want to realign this average with the center of the image. So, the calculated error is the difference between the average of the pixels belonging to the line and the center of the image. The following scheme represent and define the error : This error is used for the motor control. The operation of the motor control is explained on the next page.","title":"Error calculation"}]}