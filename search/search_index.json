{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the RoDeP doc ! \u00b6 The following documentation is related to the RoDeP project, realised in collaboration between Polytech Sorbonne and Sony CSL Paris. You will find the building instructions, and the manual for using the robot.","title":"Home"},{"location":"#welcome-to-the-rodep-doc","text":"The following documentation is related to the RoDeP project, realised in collaboration between Polytech Sorbonne and Sony CSL Paris. You will find the building instructions, and the manual for using the robot.","title":"Welcome to the RoDeP doc !"},{"location":"about/","text":"About the RoDeP project \u00b6 The RoDeP (standing for \"RObot DEplaceur de Plantes\" in French, which can be translated by PMORO (\"Plant MOver RObot\")) project is a collaboration between Sony CSL Paris and Polytech Sorbonne, a graduating school of engineering, locatedd in Paris. It has be given to students of Polytech Sorbonne in the Robotic specialty, as their industrial project, realised in cooperation with this company. Sony CSL Paris developped a plant scanner, which makes 3D model of vegetable, in order to track their growth. Putting plants, waiting for the scan to complete and then taking the plant out of the scanner is a repetitive and not funny task, that's why we developped a robot to assist the user of the scanner in his task. The first version of the robot is working, and has been tested. Project funding \u00b6 The project has been funded by Sony CSL entirely. Work team \u00b6 3 teams participated in this project. The first is the student team, including Nicolas BRANAS, Paul CHAMBROUX, Alexandre LEU and Hugo PASQUIER. It has been developped over the course of their 4th year in Polytech Sorbonne. The second team, composed by David COLLIAUX and Peter HANAPPE, is representing Sony CSL. Both of them works in the Sustainability department, which is \"commited to build tools for a sustainable society and raise awareness about the challenges of our era\". They helped the students in all kind of situation, and we thank them a lot for it. Last but not least, the Polytech Sorbonne team, with Pierre CARLES, supervised the students while they were at the school, on the technical side of the project. His support managed to drive us through the project, and we owe him a lot on the line following process. GitHub sources \u00b6 You will find every ressources for the code part of this project on the Github down below : Click the link for the project itself","title":"About"},{"location":"about/#about-the-rodep-project","text":"The RoDeP (standing for \"RObot DEplaceur de Plantes\" in French, which can be translated by PMORO (\"Plant MOver RObot\")) project is a collaboration between Sony CSL Paris and Polytech Sorbonne, a graduating school of engineering, locatedd in Paris. It has be given to students of Polytech Sorbonne in the Robotic specialty, as their industrial project, realised in cooperation with this company. Sony CSL Paris developped a plant scanner, which makes 3D model of vegetable, in order to track their growth. Putting plants, waiting for the scan to complete and then taking the plant out of the scanner is a repetitive and not funny task, that's why we developped a robot to assist the user of the scanner in his task. The first version of the robot is working, and has been tested.","title":"About the RoDeP project"},{"location":"about/#project-funding","text":"The project has been funded by Sony CSL entirely.","title":"Project funding"},{"location":"about/#work-team","text":"3 teams participated in this project. The first is the student team, including Nicolas BRANAS, Paul CHAMBROUX, Alexandre LEU and Hugo PASQUIER. It has been developped over the course of their 4th year in Polytech Sorbonne. The second team, composed by David COLLIAUX and Peter HANAPPE, is representing Sony CSL. Both of them works in the Sustainability department, which is \"commited to build tools for a sustainable society and raise awareness about the challenges of our era\". They helped the students in all kind of situation, and we thank them a lot for it. Last but not least, the Polytech Sorbonne team, with Pierre CARLES, supervised the students while they were at the school, on the technical side of the project. His support managed to drive us through the project, and we owe him a lot on the line following process.","title":"Work team"},{"location":"about/#github-sources","text":"You will find every ressources for the code part of this project on the Github down below : Click the link for the project itself","title":"GitHub sources"},{"location":"assembly/","text":"Preparing your components \u00b6 Before assembling the RoDeP, be sure having all the required components, they are enumerated in the component list . 1. Assembling the frame \u00b6 In order to assemble the frame, you have to gather the following elements : Number Component 1 frame 2 wheel 2 wheel motor 2 roulette 2 battery wedge In the end, you should have something like this : 2. Assembling the gripping part \u00b6 In order to assemble the frame, you have to gather the elements found in the gripping part of the gripper page : component list . Once you are ready, the assembling process is simple, and once it is done, the gripping part should be like this : 3. Assembling the elevation system \u00b6 In order to assemble the final part of the RoDeP, you have to gather the elements found in the second part of the gripper components : Once this is done, the physical form of the RoDeP should be finished, now is the time for getting the code work ! The code","title":"Assembly"},{"location":"assembly/#preparing-your-components","text":"Before assembling the RoDeP, be sure having all the required components, they are enumerated in the component list .","title":"Preparing your components"},{"location":"assembly/#1-assembling-the-frame","text":"In order to assemble the frame, you have to gather the following elements : Number Component 1 frame 2 wheel 2 wheel motor 2 roulette 2 battery wedge In the end, you should have something like this :","title":"1. Assembling the frame"},{"location":"assembly/#2-assembling-the-gripping-part","text":"In order to assemble the frame, you have to gather the elements found in the gripping part of the gripper page : component list . Once you are ready, the assembling process is simple, and once it is done, the gripping part should be like this :","title":"2. Assembling the gripping part"},{"location":"assembly/#3-assembling-the-elevation-system","text":"In order to assemble the final part of the RoDeP, you have to gather the elements found in the second part of the gripper components : Once this is done, the physical form of the RoDeP should be finished, now is the time for getting the code work ! The code","title":"3. Assembling the elevation system"},{"location":"component/","text":"Bill Of Material \u00b6 This chart makes an inventory of all the elements you need to build the robot. Number Component Specifications Example 1 Battery LIFEPO4 12V BMS Protection Power Battery 1 Charger NOCO GENIUS2 2 Motor driver Shield for Arduino gShield V5 2 Gripper motor Step motor Motor 2 Wheel motor Step motor Motor Encodeurs???? 1 Voltage converter 12V - 5V Rail Din adapted Voltage converter 1 Camera Raspicam 1 LED Raspicam adapted LED LED lisiparoi 3 Rail DIN fixation Rail DIN fixation 2 Controlleur Arduino uno or equivalent Arduino Uno 1 Singel board computeur Raspberry Pi 3B or equivalent Raspberry Pi 3B 2 Connector strip 5-pole Wago 2 Rail DIN's fixation for wago Fixation for wago 2 Stepper Mounting Bracket NEMA 23 1 Frame Custom made, can be found on the project Git page 2 Wheel 2 Roulette 2 Battery wedge","title":"Components list"},{"location":"component/#bill-of-material","text":"This chart makes an inventory of all the elements you need to build the robot. Number Component Specifications Example 1 Battery LIFEPO4 12V BMS Protection Power Battery 1 Charger NOCO GENIUS2 2 Motor driver Shield for Arduino gShield V5 2 Gripper motor Step motor Motor 2 Wheel motor Step motor Motor Encodeurs???? 1 Voltage converter 12V - 5V Rail Din adapted Voltage converter 1 Camera Raspicam 1 LED Raspicam adapted LED LED lisiparoi 3 Rail DIN fixation Rail DIN fixation 2 Controlleur Arduino uno or equivalent Arduino Uno 1 Singel board computeur Raspberry Pi 3B or equivalent Raspberry Pi 3B 2 Connector strip 5-pole Wago 2 Rail DIN's fixation for wago Fixation for wago 2 Stepper Mounting Bracket NEMA 23 1 Frame Custom made, can be found on the project Git page 2 Wheel 2 Roulette 2 Battery wedge","title":"Bill Of Material"},{"location":"control/","text":"Controlling the RoDeP \u00b6 The RoDeP\u2019s controls are coded in Arduino, but its routines are coded as a state machine, using the StateMachine library from Python, for easier modifications. Here we detail its use, and give some tips about how tuning this code for your use. The main problem being the line following process, we will have a deeper look on its fine tuning. The state machine \u00b6 The RoDeP state machine has the purpose of being reasonably understandable and tunable. It defines the robot as an object composed of a navigation controller (Arduino with drivers for controlling navigation motors), a gripper controller (Arduino with drivers for controlling the pliers), a camera and other useful parameters, such as masks for image treatment. With this format, the robot goes through 9 states during its mission, two of them being the initial state and final state. When starting the script, make sure to specify the number of plants to be scanned. We tried to define those states as intuitive as possible. Here is a diagram outlining its routine : Here is the detail of every state : Initializing : transitory state. At the start of the program, it initializes the serial ports for the Arduino controllers, warms up the camera, prepares the pliers, etc\u2026 It also creates, if not already existing, a log-folder that reports the line following error over time, in case of testing. Seeking : the robot follows the green line, looking for the right AprilTag to stop. Taking : the robot closes its pliers, lift up the plant, go back a little and make a half-turn. Going to scanner : following the green line again, the robot stops at scanner, lifting the plant during its journey. Waiting for scan completion : simply waits that the scan is done to start the next state. Going to storage : the robot makes a half-turn and follows the blue line, stopping at the right tag for the actual plant, the pliers going down in the process. Putting down : the robot finishes to put down its pliers and then releases its grip. Going back to scanner : the robot goes back a little, makes a half turn and follows the blue line to the scanner. It makes a half turn there, and enters its mission complete state if it has scanned the number of plants specified, otherwise it goes back to the seeking state. Mission complete : the pliers close a last time, and a end-of-mission message is displayed as a notification. Fine tuning the proportional controller \u00b6 At the moment, the navigation algorithm is controlled using a proportional controller. It suits our actual uses (2 parallel straight lines with 2 turns to the scanner; see figures about the route, average speed of 300 increments per seconds) but can be tuned for your situation, for instance increase the speed. Just keep in mind that the system converges naturally to the \u201cerror = 0\u201d state, so adding an integral gain would just slow the system; adding a derivative gain would prove useful for cancelling remaining oscillations, especially if you want to re-design the route to add more curves or increase speed, but if you get high amplitude oscillations, implementing a low-pass filter is advised, in order to cancel high-frequency noise that would be amplified by the derivative. As a reminder, when the camera takes a picture, it crops it to approximately the second quarter (from the top), and we estimate the error as the distance (on the x-axis) between the center of the cropped image and the mean value of the remaining black pixels, after image filtering. We then multiply this error value with the proportional gain, and pass the resulting value to a differential drive for the motors : As you can see, the speed given in each motor is equal to a certain proportion of the average speed of the wheels, in increments per seconds. In order to finetune the proportional gain, we give a set of tests we did in different situations. We tested 6 values in 3 different starting points, and recommend you to use the same starts in order to compare your own plots of error with ours. The graphs we give focus on the error over time regarding the line following algorithm, but beside this quantified quality, it is important to keep an acute eye over two qualitative parameters : first, the orientation of the robot at the end of the test (are the pliers in front of the plant it should be taking ? Is the robot aligned with the path ?), and secondly the \u201cempiric oscillations\u201d of the robot. As the error is measured in pixels, oscillations appearing on the graphs might not even be detectable as a human, and therefore can be insignificant. The first set of tests \u00b6 For our first set of tests, we gave the robot a step of error, and aligned it with the line it is supposed to follow, as shown on the following drawing : Testing 6 gains with this initial state gave the following results : Only one gain makes the system divergent. The others complete their task with several levels of satisfaction : though not divergent, the lowest gain of 0.001 pixels^-1 is too slow and doesn\u2019t even reach closely the order on a satisfying time, regarding our system. The gain of 0.015 pixels^-1 is also not satisfying, as it oscillates too frequently. The second set of tests \u00b6 Step of error, unaligned robot : With those tests, we evaluate the capacity of the robot to align with the line and the time it takes to do it. As expected, the lowest gain tested is too low to prevent the robot from losing the line, and diverges quickly. The gain of 0.015 pixels^-1 is still unsatisfying as the navigation enters in a \u201cpseudo-oscillatory state\u201d, and as the robot is not aligned with the line once it gets to its objective. On the scale of the system, the gain of 0.007 pixels^-1 is a good compromise between converging speed, alignment with the objective and remaining oscillations. The third set of tests \u00b6 Robot going from the scanner to the first plant. With those tests, we evaluate the capacity of the robot at reaching its effective objective in a \u201creal situation\u201d. As previously mentioned, the gain of 0.007 pixels^-1 is a good choice since it makes the robot getting to its objective correctly, with a good average time.","title":"Motor control"},{"location":"control/#controlling-the-rodep","text":"The RoDeP\u2019s controls are coded in Arduino, but its routines are coded as a state machine, using the StateMachine library from Python, for easier modifications. Here we detail its use, and give some tips about how tuning this code for your use. The main problem being the line following process, we will have a deeper look on its fine tuning.","title":"Controlling the RoDeP"},{"location":"control/#the-state-machine","text":"The RoDeP state machine has the purpose of being reasonably understandable and tunable. It defines the robot as an object composed of a navigation controller (Arduino with drivers for controlling navigation motors), a gripper controller (Arduino with drivers for controlling the pliers), a camera and other useful parameters, such as masks for image treatment. With this format, the robot goes through 9 states during its mission, two of them being the initial state and final state. When starting the script, make sure to specify the number of plants to be scanned. We tried to define those states as intuitive as possible. Here is a diagram outlining its routine : Here is the detail of every state : Initializing : transitory state. At the start of the program, it initializes the serial ports for the Arduino controllers, warms up the camera, prepares the pliers, etc\u2026 It also creates, if not already existing, a log-folder that reports the line following error over time, in case of testing. Seeking : the robot follows the green line, looking for the right AprilTag to stop. Taking : the robot closes its pliers, lift up the plant, go back a little and make a half-turn. Going to scanner : following the green line again, the robot stops at scanner, lifting the plant during its journey. Waiting for scan completion : simply waits that the scan is done to start the next state. Going to storage : the robot makes a half-turn and follows the blue line, stopping at the right tag for the actual plant, the pliers going down in the process. Putting down : the robot finishes to put down its pliers and then releases its grip. Going back to scanner : the robot goes back a little, makes a half turn and follows the blue line to the scanner. It makes a half turn there, and enters its mission complete state if it has scanned the number of plants specified, otherwise it goes back to the seeking state. Mission complete : the pliers close a last time, and a end-of-mission message is displayed as a notification.","title":"The state machine"},{"location":"control/#fine-tuning-the-proportional-controller","text":"At the moment, the navigation algorithm is controlled using a proportional controller. It suits our actual uses (2 parallel straight lines with 2 turns to the scanner; see figures about the route, average speed of 300 increments per seconds) but can be tuned for your situation, for instance increase the speed. Just keep in mind that the system converges naturally to the \u201cerror = 0\u201d state, so adding an integral gain would just slow the system; adding a derivative gain would prove useful for cancelling remaining oscillations, especially if you want to re-design the route to add more curves or increase speed, but if you get high amplitude oscillations, implementing a low-pass filter is advised, in order to cancel high-frequency noise that would be amplified by the derivative. As a reminder, when the camera takes a picture, it crops it to approximately the second quarter (from the top), and we estimate the error as the distance (on the x-axis) between the center of the cropped image and the mean value of the remaining black pixels, after image filtering. We then multiply this error value with the proportional gain, and pass the resulting value to a differential drive for the motors : As you can see, the speed given in each motor is equal to a certain proportion of the average speed of the wheels, in increments per seconds. In order to finetune the proportional gain, we give a set of tests we did in different situations. We tested 6 values in 3 different starting points, and recommend you to use the same starts in order to compare your own plots of error with ours. The graphs we give focus on the error over time regarding the line following algorithm, but beside this quantified quality, it is important to keep an acute eye over two qualitative parameters : first, the orientation of the robot at the end of the test (are the pliers in front of the plant it should be taking ? Is the robot aligned with the path ?), and secondly the \u201cempiric oscillations\u201d of the robot. As the error is measured in pixels, oscillations appearing on the graphs might not even be detectable as a human, and therefore can be insignificant.","title":"Fine tuning the proportional controller"},{"location":"control/#the-first-set-of-tests","text":"For our first set of tests, we gave the robot a step of error, and aligned it with the line it is supposed to follow, as shown on the following drawing : Testing 6 gains with this initial state gave the following results : Only one gain makes the system divergent. The others complete their task with several levels of satisfaction : though not divergent, the lowest gain of 0.001 pixels^-1 is too slow and doesn\u2019t even reach closely the order on a satisfying time, regarding our system. The gain of 0.015 pixels^-1 is also not satisfying, as it oscillates too frequently.","title":"The first set of tests"},{"location":"control/#the-second-set-of-tests","text":"Step of error, unaligned robot : With those tests, we evaluate the capacity of the robot to align with the line and the time it takes to do it. As expected, the lowest gain tested is too low to prevent the robot from losing the line, and diverges quickly. The gain of 0.015 pixels^-1 is still unsatisfying as the navigation enters in a \u201cpseudo-oscillatory state\u201d, and as the robot is not aligned with the line once it gets to its objective. On the scale of the system, the gain of 0.007 pixels^-1 is a good compromise between converging speed, alignment with the objective and remaining oscillations.","title":"The second set of tests"},{"location":"control/#the-third-set-of-tests","text":"Robot going from the scanner to the first plant. With those tests, we evaluate the capacity of the robot at reaching its effective objective in a \u201creal situation\u201d. As previously mentioned, the gain of 0.007 pixels^-1 is a good choice since it makes the robot getting to its objective correctly, with a good average time.","title":"The third set of tests"},{"location":"electriccircuit/","text":"Electrical diagram \u00b6 Here is a diagram representing the principle of the electrical planning. Fritzing document \u00b6 Here is a Fritzing capture, the Fritzing file have to be updated, but you still can study the drivers connection. Available on Github, links found on the first page of this documentation. Steppers motors \u00b6 We choose steppers motors for their high accuracy. Useful for helping the line following algorithm and gripper control. Their consumption is bigger than DC motors, as they need power when not in motion, in order to hold their position.","title":"Electric circuit"},{"location":"electriccircuit/#electrical-diagram","text":"Here is a diagram representing the principle of the electrical planning.","title":"Electrical diagram"},{"location":"electriccircuit/#fritzing-document","text":"Here is a Fritzing capture, the Fritzing file have to be updated, but you still can study the drivers connection. Available on Github, links found on the first page of this documentation.","title":"Fritzing document"},{"location":"electriccircuit/#steppers-motors","text":"We choose steppers motors for their high accuracy. Useful for helping the line following algorithm and gripper control. Their consumption is bigger than DC motors, as they need power when not in motion, in order to hold their position.","title":"Steppers motors"},{"location":"gripp/","text":"Gripper components \u00b6 On this page, you will find every components you need to assemble everything related to the gripper. The gripping part \u00b6 First of all, the list of the elements found on the gripping part is below : Number Component Specifications Example 2 linear guide 4 Attaches length 25 cm 1 Gripper motor Step motor Motor 1 belt length cm 2 cogwheel Diameter cm, teeth 2 clamping jaw 4 Rings 4 Rings for securing the opening/closing process 1 limit switch 3 Screw Reference 2 binding for linear guide 1 binding for worm screw 14 fixing bolt 4 small screw Reference M4x12 Support of the gripper \u00b6 The components for the support part is here : Number Component Specifications Example 4 linear guide length : 34,5 cm, diameter 4 mm 1 Bottom Plate Custom made The plan can be found on the Git page of the project 1 Top plate Custom made The plan can be found on the Git page of the project 1 Gripper motor Step motor Motor 1 Worm screw length : 40 cm, diameter cm 10 Clamps Internal diameter 4 mm 38 M4x12 Screw 4 washer 2 Worm screw guide 1 small cogwheel diameter cm, teeth 1 big cogwheel diameter cm, teeth","title":"Gripper system"},{"location":"gripp/#gripper-components","text":"On this page, you will find every components you need to assemble everything related to the gripper.","title":"Gripper components"},{"location":"gripp/#the-gripping-part","text":"First of all, the list of the elements found on the gripping part is below : Number Component Specifications Example 2 linear guide 4 Attaches length 25 cm 1 Gripper motor Step motor Motor 1 belt length cm 2 cogwheel Diameter cm, teeth 2 clamping jaw 4 Rings 4 Rings for securing the opening/closing process 1 limit switch 3 Screw Reference 2 binding for linear guide 1 binding for worm screw 14 fixing bolt 4 small screw Reference M4x12","title":"The gripping part"},{"location":"gripp/#support-of-the-gripper","text":"The components for the support part is here : Number Component Specifications Example 4 linear guide length : 34,5 cm, diameter 4 mm 1 Bottom Plate Custom made The plan can be found on the Git page of the project 1 Top plate Custom made The plan can be found on the Git page of the project 1 Gripper motor Step motor Motor 1 Worm screw length : 40 cm, diameter cm 10 Clamps Internal diameter 4 mm 38 M4x12 Screw 4 washer 2 Worm screw guide 1 small cogwheel diameter cm, teeth 1 big cogwheel diameter cm, teeth","title":"Support of the gripper"},{"location":"hardware_starting/","text":"Building the robot \u00b6 Building order \u00b6 The RoDeP build can be separate in 2 parts : the frame and the gripper. The first one has most of the electronics, with the battery and the boards, the second has the gripping system, used for the elevation of the plant. Therefore, both part can be assembled simulteanously, and bind together at a later point of the project. Building the frame \u00b6 As previously mentionned, the frame has most of the electronics systems of the robot. You can find the instructions in the next part. ![Rail DIN](RailDIN files/rail_DIN.png) Building the gripper \u00b6 The gripper allows the RoDeP to grab the plants, elevate them in order to scan. The instructions are found after the frame.","title":"Starting instructions"},{"location":"hardware_starting/#building-the-robot","text":"","title":"Building the robot"},{"location":"hardware_starting/#building-order","text":"The RoDeP build can be separate in 2 parts : the frame and the gripper. The first one has most of the electronics, with the battery and the boards, the second has the gripping system, used for the elevation of the plant. Therefore, both part can be assembled simulteanously, and bind together at a later point of the project.","title":"Building order"},{"location":"hardware_starting/#building-the-frame","text":"As previously mentionned, the frame has most of the electronics systems of the robot. You can find the instructions in the next part. ![Rail DIN](RailDIN files/rail_DIN.png)","title":"Building the frame"},{"location":"hardware_starting/#building-the-gripper","text":"The gripper allows the RoDeP to grab the plants, elevate them in order to scan. The instructions are found after the frame.","title":"Building the gripper"},{"location":"manual/","text":"","title":"Manual"},{"location":"rail/","text":"Rail DIN system \u00b6 A Rail DIN is a standardized support profile, generally metallic, widely used for the mechanical support of electrical equipment. The Rail DIN system allows us to assemble and disassemble easily the differents electronical components. Moreover, it offers us the possibility of changing the components whithout having to modify the frame. Rail DIN support and adaptator \u00b6 To fix the different processors and controller on the Rail DIN we use support, they allow a strong fixation avoiding all parasitic movement. Then we use homemade plastic adaptator to bind devices and supports. The assembly is complex, do not tighten the bolts too hard the first time or use : pcb spacer/support","title":"Rail DIN system"},{"location":"rail/#rail-din-system","text":"A Rail DIN is a standardized support profile, generally metallic, widely used for the mechanical support of electrical equipment. The Rail DIN system allows us to assemble and disassemble easily the differents electronical components. Moreover, it offers us the possibility of changing the components whithout having to modify the frame.","title":"Rail DIN system"},{"location":"rail/#rail-din-support-and-adaptator","text":"To fix the different processors and controller on the Rail DIN we use support, they allow a strong fixation avoiding all parasitic movement. Then we use homemade plastic adaptator to bind devices and supports. The assembly is complex, do not tighten the bolts too hard the first time or use : pcb spacer/support","title":"Rail DIN support and adaptator"},{"location":"software/","text":"","title":"Software"},{"location":"software_starting/","text":"Software \u00b6 The software part of the robot has been divided in 2 : The tracking part, and the control one. Uploading the code found on Github on your Raspberry should do the trick. The tracking part \u00b6 The explanation of the code explaining how the RoDeP follows the line will be found here. You will find the design of the playground of the robot, the process of how the robot follows the indication marked on the ground and how the error is calculated. The control \u00b6 This part contains all the information you need about the control. After a brief explanation of the differents states of the machine, you can also read on how we defined the proportionnal corrector of the robot.","title":"Starting instructions"},{"location":"software_starting/#software","text":"The software part of the robot has been divided in 2 : The tracking part, and the control one. Uploading the code found on Github on your Raspberry should do the trick.","title":"Software"},{"location":"software_starting/#the-tracking-part","text":"The explanation of the code explaining how the RoDeP follows the line will be found here. You will find the design of the playground of the robot, the process of how the robot follows the indication marked on the ground and how the error is calculated.","title":"The tracking part"},{"location":"software_starting/#the-control","text":"This part contains all the information you need about the control. After a brief explanation of the differents states of the machine, you can also read on how we defined the proportionnal corrector of the robot.","title":"The control"},{"location":"starting_instruction/","text":"Starting Instructions \u00b6 In order to see a working robot, you will need to follow those steps. 1 : Building the robot \u00b6 Building the robot is the first step to ensure the development of the project. 2 : Coding \u00b6 Now is the time to add the ![code](software.md) to the robot, so it would follow the line and take the plants. 3 : Testing the robot \u00b6 If everything is ready, now all you need is to try the robot !","title":"Starting instructions"},{"location":"starting_instruction/#starting-instructions","text":"In order to see a working robot, you will need to follow those steps.","title":"Starting Instructions"},{"location":"starting_instruction/#1-building-the-robot","text":"Building the robot is the first step to ensure the development of the project.","title":"1 : Building the robot"},{"location":"starting_instruction/#2-coding","text":"Now is the time to add the ![code](software.md) to the robot, so it would follow the line and take the plants.","title":"2 : Coding"},{"location":"starting_instruction/#3-testing-the-robot","text":"If everything is ready, now all you need is to try the robot !","title":"3 : Testing the robot"},{"location":"tracker/","text":"Purpose of the line tracker \u00b6 The line tracker allows the robot to move autonomously in its storages areas. The robot is able to follow a line and recognize Apriltags to stop when it is necessary. The RODEP's work area \u00b6 Here is an example of the orgazination of the robot's work area : The picture above shows the playground of the RoDeP. There are 2 branches, a blue and a green one, and the tags are in red. Those colors ease the work of the algorithm of movement. The tags are April Tags (or AT for the rest of the doc). The one on top is number zero, and is used to find the scanner. The green and blue ways have the numbers in opposite order, so that the robot deposite the plant the further, and grab the closer one, and avoid any collision. The green one is the path where the plants are stored, before launching the program. The grey squares are where the plants are. They are located 21 cm after the corresponding tag, for the gripper to have a good take of the pot. Each AT is separated by 10 cm, center from center. The blue path is where the plants are hoarded after the scanning operation. Here, no grey square, as the work has been done and the robot won't take the plant back. The curve has been designed to reduce the sharpness of the way, and avoid any suddent change of direction. Line tracker operation \u00b6 The Raspberry Pi Camera allows to obtain RGB images. These images are passed to HSV format in order to perform a thresholding on them using OpenCV. The principle of thresholding is to set a minimum and a maximum for the value of H, S and V. For each pixel in HSV image, the function test if the value of H, S and V are between the minimum and the maximum. If it is the case then the pixel turns to white otherwise it turns to black. Here is 3 lines of the Python code to obtain the 3 mask with the thresholding : apriltag_mask = 255-cv2.inRange(frame_HSV, (140,110,0), (180,200,255)) blue_line_mask = 255-cv2.inRange(frame_HSV, (110,170,0), (130,255,255)) green_line_mask = 255-cv2.inRange(frame_HSV, (80,60,0), (110,170,255)) For the next step it is neccessary to have the objects in black on a white background. So, this is the reason of the 255-cv2.inrange(...) to invert the colors. Error calculation \u00b6 Now that the lines are detected, the error must be defined and calculated to allow the robot to modify its trajectory. The chosen line tracking technique is the one where we take the average of the pixels belonging to the line and we want to realign this average with the center of the image. So, the calculated error is the difference between the average of the pixels belonging to the line and the center of the image. This error is used for the motor control. The operation of the motor control is explained on the next page.","title":"Line tracker"},{"location":"tracker/#purpose-of-the-line-tracker","text":"The line tracker allows the robot to move autonomously in its storages areas. The robot is able to follow a line and recognize Apriltags to stop when it is necessary.","title":"Purpose of the line tracker"},{"location":"tracker/#the-rodeps-work-area","text":"Here is an example of the orgazination of the robot's work area : The picture above shows the playground of the RoDeP. There are 2 branches, a blue and a green one, and the tags are in red. Those colors ease the work of the algorithm of movement. The tags are April Tags (or AT for the rest of the doc). The one on top is number zero, and is used to find the scanner. The green and blue ways have the numbers in opposite order, so that the robot deposite the plant the further, and grab the closer one, and avoid any collision. The green one is the path where the plants are stored, before launching the program. The grey squares are where the plants are. They are located 21 cm after the corresponding tag, for the gripper to have a good take of the pot. Each AT is separated by 10 cm, center from center. The blue path is where the plants are hoarded after the scanning operation. Here, no grey square, as the work has been done and the robot won't take the plant back. The curve has been designed to reduce the sharpness of the way, and avoid any suddent change of direction.","title":"The RODEP's work area"},{"location":"tracker/#line-tracker-operation","text":"The Raspberry Pi Camera allows to obtain RGB images. These images are passed to HSV format in order to perform a thresholding on them using OpenCV. The principle of thresholding is to set a minimum and a maximum for the value of H, S and V. For each pixel in HSV image, the function test if the value of H, S and V are between the minimum and the maximum. If it is the case then the pixel turns to white otherwise it turns to black. Here is 3 lines of the Python code to obtain the 3 mask with the thresholding : apriltag_mask = 255-cv2.inRange(frame_HSV, (140,110,0), (180,200,255)) blue_line_mask = 255-cv2.inRange(frame_HSV, (110,170,0), (130,255,255)) green_line_mask = 255-cv2.inRange(frame_HSV, (80,60,0), (110,170,255)) For the next step it is neccessary to have the objects in black on a white background. So, this is the reason of the 255-cv2.inrange(...) to invert the colors.","title":"Line tracker operation"},{"location":"tracker/#error-calculation","text":"Now that the lines are detected, the error must be defined and calculated to allow the robot to modify its trajectory. The chosen line tracking technique is the one where we take the average of the pixels belonging to the line and we want to realign this average with the center of the image. So, the calculated error is the difference between the average of the pixels belonging to the line and the center of the image. This error is used for the motor control. The operation of the motor control is explained on the next page.","title":"Error calculation"}]}